"""Parsing utilities that turn cactus-prepare output into structured plans."""
from __future__ import annotations

from datetime import datetime
import re
import shlex
from typing import Iterable, List, Optional

from .models import Plan, PrepareHeader, Round, Step


HEADER_GENERATED_RE = re.compile(r"^##\s*generated\s+by\s*:\s*(?P<value>.+)$", re.IGNORECASE)
HEADER_DATE_RE = re.compile(r"^##\s*date\s*:\s*(?P<value>.+)$", re.IGNORECASE)
HEADER_COMMIT_RE = re.compile(r"^##\s*cactus\s+commit\s*:\s*(?P<value>.+)$", re.IGNORECASE)
ROUND_HEADER_RE = re.compile(r"^###\s*Round\s+(?P<index>\d+)\s*$", re.IGNORECASE)
SECTION_PREPROC_RE = re.compile(r"^##\s*Preprocessor", re.IGNORECASE)
SECTION_ALIGNMENT_RE = re.compile(r"^##\s*Alignment", re.IGNORECASE)
SECTION_HALMERGE_RE = re.compile(r"^##\s*HAL\s+merg", re.IGNORECASE)
ANSI_ESCAPE_RE = re.compile(r"\x1b\[[0-?]*[ -/]*[@-~]")


def _strip_ansi(value: str) -> str:
    """Remove ANSI escape sequences so parsing stays robust."""

    return ANSI_ESCAPE_RE.sub("", value)


class ParseError(RuntimeError):
    """Raised when the prepare script cannot be interpreted."""


def parse_prepare_script(text: str) -> Plan:
    """Parse *text* generated by ``cactus-prepare`` into a :class:`Plan`."""

    lines = [_strip_ansi(line) for line in text.splitlines()]
    header = _parse_header(lines)
    preprocess_lines, alignment_lines, halmerge_lines = _split_sections(lines)

    preprocess_steps = _parse_command_lines(preprocess_lines, expected_kind="preprocess")
    hal_merge_steps = _parse_command_lines(halmerge_lines, default_kind="halmerge")
    rounds = _parse_rounds(alignment_lines)

    prepare_args = _extract_prepare_args(header.generated_by)

    return Plan(
        header=header,
        preprocess=preprocess_steps,
        rounds=rounds,
        hal_merges=hal_merge_steps,
        out_seq_file=prepare_args["out_seq_file"],
        out_dir=prepare_args.get("out_dir"),
    )


def _parse_header(lines: list[str]) -> PrepareHeader:
    generated_by: Optional[str] = None
    date_value: Optional[str] = None
    commit_value: Optional[str] = None

    for line in lines:
        if generated_by is None:
            match = HEADER_GENERATED_RE.match(line.strip())
            if match:
                generated_by = match.group("value").strip()
                continue
        if date_value is None:
            match = HEADER_DATE_RE.match(line.strip())
            if match:
                date_value = match.group("value").strip()
                continue
        if commit_value is None:
            match = HEADER_COMMIT_RE.match(line.strip())
            if match:
                commit_value = match.group("value").strip()
                continue
        if generated_by and date_value and commit_value is not None:
            break

    if generated_by is None:
        raise ParseError("Missing '## generated by' header")
    if date_value is None:
        raise ParseError("Missing '## date' header")

    try:
        parsed_date = datetime.fromisoformat(date_value)
    except ValueError as exc:  # pragma: no cover - bad inputs raise clear error
        raise ParseError(f"Cannot parse header date '{date_value}': {exc}") from exc

    commit_clean = commit_value if commit_value and commit_value.lower() != "none" else None

    return PrepareHeader(
        generated_by=generated_by,
        date=parsed_date,
        cactus_commit=commit_clean,
    )


def _split_sections(lines: list[str]) -> tuple[list[str], list[str], list[str]]:
    current: Optional[str] = None
    preprocess: list[str] = []
    alignment: list[str] = []
    halmerge: list[str] = []

    for line in lines:
        stripped = line.strip()
        if SECTION_PREPROC_RE.match(stripped):
            current = "preprocess"
            continue
        if SECTION_ALIGNMENT_RE.match(stripped):
            current = "alignment"
            continue
        if SECTION_HALMERGE_RE.match(stripped):
            current = "halmerge"
            continue
        if stripped.startswith("##") and not stripped.startswith("###"):
            continue
        if current == "preprocess":
            preprocess.append(line)
        elif current == "alignment":
            alignment.append(line)
        elif current == "halmerge":
            halmerge.append(line)

    return preprocess, alignment, halmerge


def _parse_command_lines(
    lines: Iterable[str],
    expected_kind: Optional[str] = None,
    default_kind: Optional[str] = None,
) -> list[Step]:
    steps: list[Step] = []
    for line in lines:
        step = _parse_line_to_step(line, expected_kind=expected_kind, default_kind=default_kind)
        if step:
            steps.append(step)
    return steps


def _parse_rounds(alignment_lines: list[str]) -> list[Round]:
    rounds: list[Round] = []
    heading = None
    pending_steps: list[tuple[str | None, Step]] = []

    for line in alignment_lines:
        stripped = line.strip()
        if not stripped:
            continue
        match = ROUND_HEADER_RE.match(stripped)
        if match:
            heading = f"Round {match.group('index')}"
            continue
        step = _parse_line_to_step(line)
        if step:
            pending_steps.append((heading, step))

    name_counts: dict[str, int] = {}
    idx = 0
    while idx < len(pending_steps):
        heading_label, step = pending_steps[idx]
        if step.kind != "blast":
            idx += 1
            continue
        blast_step = step
        idx += 1
        if idx >= len(pending_steps):
            raise ParseError("Missing cactus-align after cactus-blast")
        align_heading, align_step = pending_steps[idx]
        if align_step.kind != "align":
            raise ParseError(
                f"Expected cactus-align after cactus-blast, got: {align_step.raw}"
            )
        idx += 1
        hal_steps: list[Step] = []
        while idx < len(pending_steps) and pending_steps[idx][1].kind == "hal2fasta":
            hal_steps.append(pending_steps[idx][1])
            idx += 1

        base_name = heading_label or align_heading or blast_step.root or f"Round {len(rounds)}"
        if base_name.startswith("Round"):
            name_counts.setdefault(base_name, 0)
            suffix_index = name_counts[base_name]
            name_counts[base_name] += 1
            if suffix_index == 0:
                round_name = base_name
            else:
                root_label = align_step.root or blast_step.root or f"part{suffix_index}"
                round_name = f"{base_name} ({root_label})"
        else:
            round_name = base_name

        target_hal = _pick_align_hal(align_step)
        rounds.append(
            Round(
                name=round_name,
                root=align_step.root or blast_step.root or round_name,
                target_hal=target_hal,
                blast_step=blast_step,
                align_step=align_step,
                hal2fasta_steps=hal_steps,
            )
        )
    return rounds





def _pick_align_hal(align_step: Step) -> str:
    for outfile in align_step.out_files:
        if outfile.endswith(".hal"):
            return outfile
    tokens = shlex.split(align_step.raw)
    for token in reversed(tokens):
        if token.endswith(".hal"):
            return token
    raise ParseError(f"Cannot determine target HAL from align command: {align_step.raw}")


def _parse_line_to_step(
    line: str,
    expected_kind: Optional[str] = None,
    default_kind: Optional[str] = None,
) -> Optional[Step]:
    stripped = line.strip()
    if not stripped:
        return None
    if stripped.startswith("##"):
        return None
    tokens = _safe_split(stripped)
    if not tokens:
        return None
    kind = _classify_kind(tokens[0], default_kind)
    if expected_kind and kind != expected_kind:
        kind = expected_kind
    jobstore = _extract_jobstore(tokens)
    root = _extract_root(tokens, kind)
    log_file = _extract_log_file(tokens)
    out_files = _extract_outputs(tokens, kind)
    return Step(
        raw=stripped,
        kind=kind,
        jobstore=jobstore,
        out_files=out_files,
        root=root,
        log_file=log_file,
    )


def _safe_split(command: str) -> List[str]:
    try:
        return shlex.split(command)
    except ValueError as exc:
        raise ParseError(f"Cannot parse command: {command}\n{exc}") from exc


def _classify_kind(first_token: str, default_kind: Optional[str]) -> str:
    mapping = {
        "cactus-preprocess": "preprocess",
        "cactus-blast": "blast",
        "cactus-align": "align",
        "hal2fasta": "hal2fasta",
        "halAppendSubtree": "halmerge",
        "RaMAx": "ramax",
    }
    kind = mapping.get(first_token)
    if kind:
        return kind
    if default_kind:
        return default_kind
    return "other"


def _extract_jobstore(tokens: list[str]) -> Optional[str]:
    for idx, tok in enumerate(tokens):
        if tok.startswith("jobstore/"):
            return tok
        if tok in {"--jobStore", "--jobstore"} and idx + 1 < len(tokens):
            return tokens[idx + 1]
    return None


def _extract_root(tokens: list[str], kind: str) -> Optional[str]:
    if kind == "halmerge" and len(tokens) >= 4:
        return tokens[3]
    for idx, tok in enumerate(tokens):
        if tok == "--root" and idx + 1 < len(tokens):
            return tokens[idx + 1]
    return None


def _extract_log_file(tokens: list[str]) -> Optional[str]:
    for idx, tok in enumerate(tokens):
        if tok in {"--logFile", "--logfile"} and idx + 1 < len(tokens):
            return tokens[idx + 1]
    return None


def _extract_outputs(tokens: list[str], kind: str) -> list[str]:
    outputs: list[str] = []
    def _maybe_add(token: str) -> None:
        if any(token.endswith(suffix) for suffix in (".paf", ".hal", ".fa", ".fa.gz", ".fasta", ".txt")):
            outputs.append(token)

    if kind == "blast":
        for token in tokens:
            if token.endswith(".paf"):
                outputs.append(token)
    elif kind in {"align", "ramax"}:
        for token in tokens:
            if token.endswith(".hal"):
                outputs.append(token)
    elif kind == "hal2fasta":
        for idx, token in enumerate(tokens):
            if token in {">", ">>"} and idx + 1 < len(tokens):
                outputs.append(tokens[idx + 1])
            elif token.endswith((".fa", ".fa.gz", ".fasta")):
                outputs.append(token)
    elif kind == "halmerge":
        for token in tokens:
            if token.endswith(".hal"):
                outputs.append(token)
    else:
        for token in tokens:
            _maybe_add(token)
    return outputs


def _extract_prepare_args(command: str) -> dict[str, str]:
    tokens = _safe_split(command)
    results: dict[str, str] = {}
    flag_to_key = {"--outSeqFile": "out_seq_file", "--outDir": "out_dir", "--outHal": "out_hal"}
    idx = 0
    while idx < len(tokens):
        token = tokens[idx]
        if token in flag_to_key and idx + 1 < len(tokens):
            results[flag_to_key[token]] = tokens[idx + 1]
            idx += 2
            continue
        idx += 1
    if "out_seq_file" not in results:
        raise ParseError("Unable to extract --outSeqFile from cactus-prepare command")
    return results
